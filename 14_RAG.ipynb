{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxeCK6gn/FqhK+ge+jqfjP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shriansh16/LLM_Engineering/blob/main/14_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qTWzI7h7WGf5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from langchain.document_loaders import PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.vectorstores import FAISS\n",
        "import numpy as np\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "zQQY09x-Wj8W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gpt-4o-mini\"\n",
        "db_name = \"vector_db\""
      ],
      "metadata": {
        "id": "f1Gn9i3xXO4q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', '')"
      ],
      "metadata": {
        "id": "HCklDwkzXVI7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path='/content/ok'\n",
        "loader = DirectoryLoader(pdf_path, glob='*.pdf', loader_cls=PyPDFLoader)\n",
        "document = loader.load()"
      ],
      "metadata": {
        "id": "wq-en7BgXZ5c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(document)"
      ],
      "metadata": {
        "id": "rfGKFA0oYUrS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sy-nJweYbxS",
        "outputId": "85980e7b-88a6-41a4-d13f-9394d277e70e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "wf0HGTbLYzZs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
        "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QP2xeBpYhdT",
        "outputId": "92586a92-0a89-482c-aa37-32b67779ae9f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorstore created with 19 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Time to use LangChain to bring it all together"
      ],
      "metadata": {
        "id": "UN2K0MjoY-tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new Chat with OpenAI\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
        "\n",
        "# set up the conversation memory for the chat\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr6UWMqtYwr0",
        "outputId": "2fdaf484-8096-486f-c1b4-65c8c10414f7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-51939673df05>:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what si smartcookie\"\n",
        "result = conversation_chain.invoke({\"question\":query})\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6lAYnFFZDS3",
        "outputId": "e4739916-1ca2-4677-a915-ee64f0d3c8c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smart Cookie is a teacher-student reward program designed to foster a fun, interactive, and rewarding environment for both teachers and students. The program allows teachers to recognize and reward students for their achievements in various activities, such as sports, drawing, and academic tests. It aims to make the educational process more engaging and motivating by providing real-time rewards that acknowledge students' efforts and accomplishments. It was founded by Avinash Kulkarni.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Now we will bring this up in Gradio using the Chat interface -"
      ],
      "metadata": {
        "id": "SmnvHWvSZau-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapping that in a function\n",
        "\n",
        "def chat(message, history):\n",
        "    result = conversation_chain.invoke({\"question\": message})\n",
        "    return result[\"answer\"]"
      ],
      "metadata": {
        "id": "2Bqj8cg0ZGtH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view = gr.ChatInterface(chat).launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "ibnGeYDGZddq",
        "outputId": "56712c46-e96d-4ef9-deda-4cd6bd765c17"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:228: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://75778230e9fc56775f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://75778230e9fc56775f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tw7NO_kSZgbq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}